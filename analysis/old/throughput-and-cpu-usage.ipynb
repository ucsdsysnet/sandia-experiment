{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ccbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, sys, argparse, glob\n",
    "import csv\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import *\n",
    "from matplotlib_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6608242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeprefix(self: str, prefix: str, /) -> str:\n",
    "    if self.startswith(prefix):\n",
    "        return self[len(prefix):]\n",
    "    else:\n",
    "        return self[:]\n",
    "\n",
    "def removesuffix(self: str, suffix: str, /) -> str:\n",
    "    # suffix='' should not call self[:-0].\n",
    "    if suffix and self.endswith(suffix):\n",
    "        return self[:-len(suffix)]\n",
    "    else:\n",
    "        return self[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3077c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_prefix(l):\n",
    "    \"Given a list of pathnames, returns the longest common leading component\"\n",
    "    if not l: return ''\n",
    "    s1 = min(l)\n",
    "    s2 = max(l)\n",
    "    for i, c in enumerate(s1):\n",
    "        if c != s2[i]:\n",
    "            return s1[:i]\n",
    "    return s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cdf_array(array, label, include_count = False, index=0, color=None):\n",
    "    x = sorted(array)\n",
    "    y = np.linspace(0., 1., len(array) + 1)[1:]\n",
    "    if include_count:\n",
    "        label += ' (%d)' % len(array)\n",
    "    if color is None:\n",
    "        color = get_next_color()\n",
    "    plt.plot(x, y, label=label, color=color, linestyle=get_linestyle(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f883331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(data_array, plot_axis=None, timestamp_column_name='timestamp', prefix=None, use_relative_time=False, color=None, index=0, marker=None):\n",
    "    x = [entry[timestamp_column_name] for entry in data_array]\n",
    "    if use_relative_time:\n",
    "        start_time = x[0]\n",
    "        x = [(t - start_time).total_seconds() for t in x]\n",
    "    data_keys = []\n",
    "    for key in data_array[0].keys():\n",
    "        if key == timestamp_column_name:\n",
    "            continue\n",
    "        data_keys.append(key)\n",
    "    lines = []\n",
    "    for key in data_keys:\n",
    "        data_series = [entry[key] for entry in data_array]\n",
    "        label = ('%s - ' % prefix if prefix else '') + key\n",
    "        if plot_axis is None:\n",
    "            plot_axis = plt.gca()\n",
    "        line = plot_axis.plot(x, data_series, color=color, linestyle=get_linestyle(index), label=label, marker=marker)\n",
    "        # index += 1\n",
    "        lines.append(line[0])\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_throughput_data(throughput_log_file):\n",
    "    data = []\n",
    "    regex_pattern = re.compile(r'parallel: (\\d+), sender: ([\\d.]+), receiver: ([\\d.]+)')\n",
    "    with open(throughput_log_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if not line.startswith('parallel: '):\n",
    "                continue\n",
    "            m = regex_pattern.match(line)\n",
    "            assert m, \"Cannot parse throughput line: %s\" % line\n",
    "            data.append({\n",
    "                'parallelism': int(m.group(1)),\n",
    "                'throughput': float(m.group(2)),\n",
    "            })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpu_mem_usage_data(usage_log_file):\n",
    "    data_array = []\n",
    "    with open(usage_log_file, 'r') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        column_names = csv_reader.fieldnames\n",
    "        required_columns = set(['timestamp', 'cpu-user', 'cpu-kernel', 'cpu-idle', 'mem-used', 'mem-free'])\n",
    "        assert [required_column in column_names for required_column in required_columns]\n",
    "        for row in csv_reader:\n",
    "            timestamp = datetime.fromisoformat(row['timestamp'])\n",
    "            cpu_user = float(row['cpu-user'])\n",
    "            cpu_kernel = float(row['cpu-kernel'])\n",
    "            cpu_idle = float(row['cpu-idle'])\n",
    "            mem_used = float(row['mem-used'])\n",
    "            mem_free = float(row['mem-free'])\n",
    "            if mem_used + mem_free < 1:\n",
    "                print(row, file=sys.stderr)\n",
    "            data_array.append({\n",
    "                'timestamp': timestamp,\n",
    "                'cpu-total': cpu_user + cpu_kernel,\n",
    "                # 'mem': mem_used / (mem_used + mem_free),\n",
    "                'cpu-user': cpu_user,\n",
    "                'cpu-kernel': cpu_kernel,\n",
    "                # 'cpu-idle': cpu_idle,\n",
    "                # 'mem-used': mem_used,\n",
    "                # 'mem-free': mem_free,\n",
    "            })\n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_array is a list of dicts, each of which has 'timestamp', 'total_intel_energy', ...\n",
    "# it's already sorted by timestamp\n",
    "def get_energy_stats(data_array):\n",
    "    assert len(data_array) > 2, \"Time series is too short\"\n",
    "    l_timestamp = np.array([entry['timestamp'] for entry in data_array])\n",
    "    delta_timestamps = [int(delta.total_seconds()) for delta in np.diff(l_timestamp, n=1)]\n",
    "    sample_interval_s = delta_timestamps[0]\n",
    "    assert all(delta == sample_interval_s for delta in delta_timestamps)\n",
    "#     print('Sample interval: %ds' % sample_interval_s)\n",
    "\n",
    "    # detect idle power draw\n",
    "    l_power = np.array([entry['total_intel_energy']/sample_interval_s for entry in data_array])\n",
    "    # print(l_power)\n",
    "    delta_power = np.diff(l_power, n=1)\n",
    "    # print(delta_power)\n",
    "    assert len(l_power) == len(delta_power) + 1\n",
    "    POWER_DIFF_THRESHOLD = 1\n",
    "    IDLE_POWER_STD_THRESHOLD = 0.01\n",
    "    index_workload_start = np.argmax(delta_power > POWER_DIFF_THRESHOLD)\n",
    "    index_workload_end = len(delta_power) - np.argmax(delta_power[::-1] < -POWER_DIFF_THRESHOLD)\n",
    "    l_power_idle_before = l_power[:index_workload_start]\n",
    "    l_power_workload = l_power[index_workload_start:index_workload_end]\n",
    "    l_power_idle_after = l_power[index_workload_end:]\n",
    "    avg_power_idle_before = np.average(l_power_idle_before)\n",
    "    std_power_idle_before = np.std(l_power_idle_before)\n",
    "    avg_power_idle_after = np.average(l_power_idle_after)\n",
    "    std_power_idle_after = np.std(l_power_idle_after)\n",
    "    # print(avg_power_idle_before, std_power_idle_before, avg_power_idle_after, std_power_idle_after)\n",
    "\n",
    "    assert std_power_idle_before / avg_power_idle_before < IDLE_POWER_STD_THRESHOLD, \"Idle power std is too high\"\n",
    "    assert std_power_idle_after / avg_power_idle_after < IDLE_POWER_STD_THRESHOLD, \"Idle power std is too high\"\n",
    "    \n",
    "    # \"Idle power before/after difference is too high\"\n",
    "    avg_power_idle = np.average([avg_power_idle_before, avg_power_idle_after])\n",
    "    if np.abs(avg_power_idle_before - avg_power_idle_after) > POWER_DIFF_THRESHOLD:\n",
    "        avg_power_idle = avg_power_idle_before\n",
    "\n",
    "#     print('Workload duration: %ds' % len(l_power_workload) * sample_interval_s)\n",
    "#     print('Idle power: %.fW' % (avg_power_idle / sample_interval_s))\n",
    "    return {\n",
    "        'duration': len(l_power_workload) * sample_interval_s,\n",
    "        'start_index': index_workload_start,\n",
    "        'total_energy': np.sum(l_power_workload),\n",
    "        'delta_energy': np.sum(l_power_workload) - len(l_power_workload) * avg_power_idle,\n",
    "        'sample_interval_s': sample_interval_s,\n",
    "        'idle_power': avg_power_idle,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea57d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_detect_log_files(dirpath):\n",
    "    dirpath = os.path.expanduser(dirpath)\n",
    "    THROUGHPUT_LOGFILE_SUFFIX = \".throughput.log\"\n",
    "    USAGE_LOGFILE_SUFFIX = \".usage.csv\"\n",
    "    throughput_log_files = sorted(glob.glob(os.path.join(dirpath, '*' + THROUGHPUT_LOGFILE_SUFFIX)))\n",
    "    usage_log_files = sorted(glob.glob(os.path.join(dirpath, '*' + USAGE_LOGFILE_SUFFIX)))\n",
    "    assert len(throughput_log_files) == len(usage_log_files)\n",
    "    assert [removesuffix(filename, THROUGHPUT_LOGFILE_SUFFIX) for filename in throughput_log_files] == [removesuffix(filename, USAGE_LOGFILE_SUFFIX) for filename in usage_log_files]\n",
    "    return list(zip(throughput_log_files, usage_log_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_throughput_to_usage_data(data_usage, data_throughput):\n",
    "    # print(data_usage)\n",
    "    l_usage_nonempty = [x['cpu-total'] > 0.5 for x in data_usage]\n",
    "    range_start = -1\n",
    "    l_range_indices = []\n",
    "    for i in range(len(l_usage_nonempty)):\n",
    "        if l_usage_nonempty[i] and range_start == -1:\n",
    "            range_start = i\n",
    "        elif not l_usage_nonempty[i] and range_start != -1 and i > range_start + 10:\n",
    "            range_end = i\n",
    "            l_range_indices.append((range_start, range_end))\n",
    "            range_start = -1\n",
    "        else:\n",
    "            pass\n",
    "    # print(l_range_indices)\n",
    "    # print(data_throughput)\n",
    "    assert len(l_range_indices) == len(data_throughput), \"Failed to align throughput to usage timeseries data\"\n",
    "    data_throughput_aligned = []\n",
    "    index = 0\n",
    "    for entry in sorted(data_throughput, key=lambda e: e['parallelism']):\n",
    "        (range_start, range_end) = l_range_indices[index]\n",
    "        index += 1\n",
    "        range_middle_index = (range_start + range_end) // 2\n",
    "        timestamp_aligned = data_usage[range_middle_index]['timestamp']\n",
    "        data_throughput_aligned.append({\n",
    "            'timestamp': timestamp_aligned,\n",
    "            'throughput': entry['throughput']\n",
    "        })\n",
    "    return data_throughput_aligned\n",
    "\n",
    "# align_throughput_to_usage_data(data_sender_usage, data_throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4310f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    'iperf-1-16',\n",
    "    'iperf-32'\n",
    "]\n",
    "\n",
    "ROOT_DIR = os.path.relpath('../logs/with-tc')\n",
    "\n",
    "figsizes = [\n",
    "    (10, 6),\n",
    "    None\n",
    "]\n",
    "experiment_index = 0\n",
    "for experiment in experiments:\n",
    "    figsize = figsizes[experiment_index]\n",
    "    experiment_index += 1\n",
    "    filename_sender_usage = '%s.sender.usage.csv' % experiment\n",
    "    filename_receiver_usage = '%s.receiver.usage.csv' % experiment\n",
    "    filename_throughput = '%s.throughput.log' % experiment\n",
    "    data_sender_usage = get_cpu_mem_usage_data(os.path.join(ROOT_DIR, filename_sender_usage))\n",
    "    data_receiver_usage = get_cpu_mem_usage_data(os.path.join(ROOT_DIR, filename_receiver_usage))\n",
    "    data_throughput = get_throughput_data(os.path.join(ROOT_DIR, filename_throughput))\n",
    "    # print(data_throughput)\n",
    "    for (data_usage, prefix) in [(data_sender_usage, \"sender\"), (data_receiver_usage, \"receiver\")]:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax1 = fig.subplots()\n",
    "        ax2 = ax1.twinx()\n",
    "        lines1 = plot_timeseries(data_usage, prefix=None, plot_axis=ax1)\n",
    "        data_throughput_aligned = align_throughput_to_usage_data(data_usage, data_throughput)\n",
    "        # print(data_throughput_aligned)\n",
    "        lines2 = plot_timeseries(data_throughput_aligned, prefix=None, plot_axis=ax2, color='black', index = 1, marker='o')\n",
    "        ax1.set_ylim(0, 10)\n",
    "        ax1.set_xlabel('Time')\n",
    "        ax1.set_ylabel('CPU usage (%)')\n",
    "        ax2.set_ylim(0, 100)\n",
    "        ax2.set_ylabel('Throughput (Gbps)')\n",
    "        plt.grid(which='major', linestyle='-')\n",
    "        # plt.grid(which='minor', linestyle='-.')\n",
    "        plt.title('%s - %s' % (experiment, prefix))\n",
    "        lines = (lines1 + lines2)\n",
    "        labels = [line.get_label() for line in lines]\n",
    "        ax1.legend(lines, labels)\n",
    "        plt.savefig('%s.%s.png' % (experiment, prefix))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c95a14b0b15b2dff0ac63cf46f19a415aa466114342573829c0ea35de1c1134a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py39')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
